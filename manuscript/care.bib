
@book{MarkBook,
	Address = {New York Dordrecht Heidelberg London},
	Author = {van der Laan, M. and Rose, S.},
	Publisher = {Springer},
	Title = {Targeted Learning: Causal Inference for Observational and Experimental Data},
	Year = {2011}}


@article{Holland1986,
	Author = {Holland, P.W.},
	Journal = {{Journal of the American Statistical Association}},
	Number = {396},
	Pages = {945--960},
	Title = {Statistics and Causal Inference},
	Volume = {81},
	Year = {1986}}

@book{Spirtes1993,
	Address = {New York/Berlin},
	Author = {P. Spirtes and C. Glymour and R. Scheines},
	Publisher = {Springer-Verlag},
	Title = {Causation, Prediction and Search. Number 81 in Lecture Notes in Statistics},
	Year = {1993}}

@incollection{Robins2009,
	Address = {Boca Raton, FL},
	Author = {J.M. Robins and M.A. Hern\'{a}n},
	Booktitle = {{Longitudinal Data Analysis}},
	Chapter = {23},
	Editor = {G. Fitzmaurice and M. Davidian and G. Verbeke and G. Molenberghs},
	Publisher = {Chapman \& Hall/CRC},
	Title = {{Estimation of the causal effects of time-varying exposures}},
	Year = {2009}}


@article{Hernan2016,
	Author = {M.A. Hern\'{a}n and J.M. Robins},
	Journal = {American Journal of Epidemiology},
	Number = {8},
	Pages = {758-764},
	Title = {Using Big Data to Emulate a Target Trial When a Randomized Trial Is Not Available},
	Volume = {183},
	Year = {2016}}

@techreport{Richardson2013swig,
	Author = {T.S. Richardson and J.M. Robins},
	Institution = {Center for Statistics and the Social Sciences University of Washington},
	Title = {Single World Intervention Graphs ({SWIG}s): A Unification of the Counterfactual and Graphical Approaches to Causality},
	Type = {Working Paper Number 128},
	Year = {2013}}



@article{Balzer2016,
abstract = {In randomized trials, adjustment for measured covariates during the analysis can reduce variance and increase power. To avoid misleading inference, the analysis plan must be pre-specified. However, it is often unclear a priori which baseline covariates (if any) should be adjusted for in the analysis. Consider, for example, the Sustainable East Africa Research in Community Health (SEARCH) trial for HIV prevention and treatment. There are 16 matched pairs of communities and many potential adjustment variables, including region, HIV prevalence, male circumcision coverage, and measures of community-level viral load. In this paper, we propose a rigorous procedure to data-adaptively select the adjustment set, which maximizes the efficiency of the analysis. Specifically, we use cross-validation to select from a pre-specified library the candidate targeted maximum likelihood estimator (TMLE) that minimizes the estimated variance. For further gains in precision, we also propose a collaborative procedure for estimating the known exposure mechanism. Our small sample simulations demonstrate the promise of the methodology to maximize study power, while maintaining nominal confidence interval coverage. We show how our procedure can be tailored to the scientific question (intervention effect for the study sample vs. for the target population) and study design (pair-matched or not). Copyright {\textcopyright} 2016 John Wiley {\&} Sons, Ltd.},
author = {Balzer, Laura B and van der Laan, Mark J and Petersen, Maya L and {SEARCH Collaboration}, the SEARCH},
doi = {10.1002/sim.7023},
issn = {1097-0258},
journal = {Statistics in medicine},
keywords = {causal inference,covariate selection,data-adaptive,pair-matched,randomized trials,targeted maximum likelihood estimation (TMLE)},
mendeley-groups = {CARE-paper},
number = {25},
pages = {4528--4545},
pmid = {27436797},
publisher = {NIH Public Access},
title = {Adaptive pre-specification in randomized trials with and without pair-matching.},
volume = {35},
year = {2016}
}
@article{Balzer2016a,
abstract = {In cluster randomized trials, the study units usually are not a simple random sample from some clearly defined target population. Instead, the target population tends to be hypothetical or ill?defined, and the selection of study units tends to be systematic, driven by logistical and practical considerations. As a result, the population average treatment effect (PATE) may be neither well defined nor easily interpretable. In contrast, the sample average treatment effect (SATE) is the mean difference in the counterfactual outcomes for the study units. The sample parameter is easily interpretable and arguably the most relevant when the study units are not sampled from some specific super?population of interest. Furthermore, in most settings, the sample parameter will be estimated more efficiently than the population parameter. To the best of our knowledge, this is the first paper to propose using targeted maximum likelihood estimation (TMLE) for estimation and inference of the sample effect in trials with and without pair?matching. We study the asymptotic and finite sample properties of the TMLE for the sample effect and provide a conservative variance estimator. Finite sample simulations illustrate the potential gains in precision and power from selecting the sample effect as the target of inference. This work is motivated by the Sustainable East Africa Research in Community Health (SEARCH) study, a pair?matched, community randomized trial to estimate the effect of population?based HIV testing and streamlined ART on the 5?year cumulative HIV incidence (NCT01864603). The proposed methodology will be used in the primary analysis for the SEARCH trial. Copyright {\textcopyright} 2016 John Wiley {\&} Sons, Ltd.},
author = {Balzer, Laura B. and Petersen, Maya L. and van der Laan, Mark J. and the {SEARCH Collaboration}},
doi = {10.1002/sim.6965},
issn = {10970258},
journal = {Statistics in Medicine},
keywords = {cluster randomized trials,pair-matching,population average treatment effect (PATE),sample average treatment effect (SATE),targeted maximum likelihood estimation (TMLE)},
mendeley-groups = {CARE-paper},
title = {Targeted estimation and inference for the sample average treatment effect in trials with and without pair-matching},
year = {2016}
}
@article{Bareinboim2013,
abstract = {Generalizing empirical findings to new environments, settings, or populations is essential in most scientific explorations. This article treats a particular problem of generalizability, called "transportability", defined as a license to transfer information learned in experimental studies to a different population, on which only observational studies can be conducted. Given a set of assumptions concerning commonalities and differences between the two populations, Pearl and Bareinboim (2011) derived sufficient conditions that permit such transfer to take place. This article summarizes their findings and supplements them with an effective procedure for deciding when and how transportability is feasible. It establishes a necessary and sufficient condition for deciding when causal effects in the target population are estimable from both the statistical information available and the causal information transferred from the experiments. The article further provides a complete algorithm for computing the transport formula, that is, a way of combining observational and experimental information to synthesize bias-free estimate of the desired causal relation. Finally, the article examines the differences between transportability and other variants of generalizability.},
archivePrefix = {arXiv},
arxivId = {1312.7485},
author = {Bareinboim, Elias and Pearl, Judea},
doi = {10.1515/jci-2012-0004},
eprint = {1312.7485},
isbn = {2193-3677},
issn = {2193-3685},
journal = {Journal of Causal Inference},
mendeley-groups = {CARE-paper},
number = {1},
pages = {107--134},
title = {A General Algorithm for Deciding Transportability of Experimental Results},
volume = {1},
year = {2013}
}
@article{Bastian2010,
abstract = {Hilda Bastian and colleagues examine the extent to which critical summaries of clinical trials can be used by health professionals and the public.},
author = {Bastian, Hilda and Glasziou, Paul and Chalmers, Iain},
doi = {10.1371/journal.pmed.1000326},
issn = {1549-1676},
journal = {PLoS Medicine},
mendeley-groups = {CARE-paper},
month = sep,
number = {9},
pages = {e1000326},
publisher = {Public Library of Science},
title = {Seventy-Five Trials and Eleven Systematic Reviews a Day: How Will We Ever Keep Up?},
volume = {7},
year = {2010}
}
@article{Bennett2002,
author = {Bennett, Steve and Parpia, Tamiza and Hayes, Richard and Cousens, Simon},
doi = {10.1093/ije/31.4.839},
issn = {1464-3685},
journal = {International Journal of Epidemiology},
mendeley-groups = {CARE-paper},
month = aug,
number = {4},
pages = {839--846},
publisher = {Oxford University Press},
title = {Methods for the analysis of incidence rates in cluster randomized trials},
volume = {31},
year = {2002}
}
@article{Binka1996,
abstract = {A community-based randomized, controlled trial of permethrin impregnated bednets was carried out in a rural area of northern Ghana, between July 1993 and June 1995, to assess the impact on the mortality of young children in an area of intense transmission of malaria and no tradition of bednet use. The district around Navrongo was divided into 96 geographical areas and in 48 randomly selected areas households were provided with permethrin impregnated bednets which were re-impregnated every 6 months. A longitudinal demographic surveillance system was used to record births, deaths and migrations, to evaluate compliance and to measure child mortality. The use of permethrin impregnated bednets was associated with 17{\%} reduction in all-cause mortality in children aged 6 months to 4 years (RR = 0.83; 95{\%} CI 0.69-1.00; P = 0.05). The reduction in mortality was confined to children aged 2 years of younger, and was greater in July-December, during the wet season and immediately after (RR = 0.79; 95{\%} CI 0.63-1.00), a period when malaria mortality is likely to be increased, than in the dry season (RR = 0.92, 95{\%} CI 0.73-1.14). The ready acceptance of bednets, the high level of compliance in their use and the subsequent impact on all-cause mortality in this study has important implications for programmes to control malaria in sub-Saharan Africa.},
author = {Binka, F. N. and Kubaje, A. and Adjuik, M. and Williams, L. A. and Lengeler, C. and Maude, G. H. and Armah, G. E. and Kajihara, B. and Adiamah, J. H. and Smith, P. G.},
doi = {10.1111/j.1365-3156.1996.tb00020.x},
isbn = {1360-2276},
issn = {13602276},
journal = {Tropical Medicine {\&} International Health},
mendeley-groups = {CARE-paper},
number = {2},
pages = {147--154},
pmid = {8665378},
title = {Impact of permethrin impregnated bednets on child mortality in Kassena-Nankana district, Ghana: a randomized controlled trial},
volume = {1},
year = {1996}
}
@article{Blackwell2002,
abstract = {Abstract Body condition in small mammals is often investigated by examining the residuals of a regression of mass on a linear measure of body size. Such indices include a number of assumptions, which, in most cases, are not tested. This study presents a residual condition index, that uses the regression of body mass on the Axis 1 factor scores from a Principal Components Analysis, to estimate body condition in free?living ship rats, Rattus rattus Linneaus 1758, caught in mixed forest in New Zealand. The results from this index and a simpler ratio index were compared to determine how well they met the assumptions of the technique, and how well they identified biological differences in the population. Of the indices tested, the residual index based on an Ordinary Least Squared regression proved to be the most statistically robust method. The condition indices calculated were probably correlated with significant differences between individuals, but were not compared with true condition as estimated by fat re...},
author = {Blackwell, G. L.},
doi = {10.1080/03014223.2002.9518303},
issn = {0301-4223},
journal = {New Zealand Journal of Zoology},
keywords = {Principal Components Analysis,Rattus rattus,body condition,linear regression,residual index,ship rat},
mendeley-groups = {CARE-paper},
month = jan,
number = {3},
pages = {195--203},
publisher = { Taylor {\&} Francis Group },
title = {A potential multivariate index of condition for small mammals},
volume = {29},
year = {2002}
}
@article{Ceyhan2009,
abstract = {Various methods to control the influence of a covariate on a response variable are compared. In particular, ANOVA with or without homogeneity of variances (HOV) of errors and Kruskal-Wallis (K-W) tests on covariate-adjusted residuals and analysis of covariance (ANCOVA) are compared. Covariate-adjusted residuals are obtained from the overall regression line fit to the entire data set ignoring the treatment levels or factors. The underlying assumptions for ANCOVA and methods on covariate-adjusted residuals are determined and the methods are compared only when both methods are appropriate. It is demonstrated that the methods on covariate-adjusted residuals are only appropriate in removing the covariate influence when the treatment-specific lines are parallel and treatment-specific covariate means are equal. Empirical size and power performance of the methods are compared by extensive Monte Carlo simulations. We manipulated the conditions such as assumptions of normality and HOV, sample size, and clustering of the covariates. The parametric methods (i.e., ANOVA with or without HOV on covariate-adjusted residuals and ANCOVA) exhibited similar size and power when error terms have symmetric distributions with variances having the same functional form for each treatment, and covariates have uniform distributions within the same interval for each treatment. For large samples, it is shown that the parametric methods will give similar results if sample covariate means for all treatments are similar. In such cases, parametric tests have higher power compared to the nonparametric K-W test on covariate-adjusted residuals. When error terms have asymmetric distributions or have variances that are heterogeneous with different functional forms for each treatment, ANCOVA and analysis of covariate-adjusted residuals are liberal with K-W test having higher power than the parametric tests. The methods on covariate-adjusted residuals are severely affected by the clustering of the covariates relative to the treatment factors, when covariate means are very different for treatments. For data clusters, ANCOVA method exhibits the appropriate level. However such a clustering might suggest dependence between the covariates and the treatment factors, so makes ANCOVA less reliable as well. Guidelines on which method to use for various cases are also provided.},
archivePrefix = {arXiv},
arxivId = {arXiv:0903.4331v2},
author = {Ceyhan, Elvan and Goad, Carla L},
eprint = {arXiv:0903.4331v2},
mendeley-groups = {CARE-paper},
title = {A Comparison of Analysis of Covariate-Adjusted Residuals and Analysis of Covariance},
year = {2009}
}
@article{Cochran1957,
author = {Cochran, William G.},
doi = {10.2307/2527916},
issn = {0006341X},
journal = {Biometrics},
mendeley-groups = {CARE-paper},
month = sep,
number = {3},
pages = {261},
publisher = {International Biometric Society},
title = {Analysis of Covariance: Its Nature and Uses},
volume = {13},
year = {1957}
}
@article{Cochran1973,
abstract = {This paper reviews work on the effectiveness of different methods of matched sampling and statistical adjustment, alone and in combination, in reducing bias due to confounding x-variables when comparing two populations. The adjustment methods were linear regression adjustment for x continuous and direct standardization for x categorical. With x continuous, the range of situations examined included linear relations between y and x, parallel and non-parallel, monotonic non-linear parallel relations, equal and unequal variances of x, and the presence of errors of measurement in x. The percent of initial bias {\$}E(\backslashoverline{y}{\_}{1}-\backslashoverline{y}{\_}{2}){\$} that was removed was used as the criterion. Overall, linear regression adjustment on random samples appeared superior to the matching methods, with linear regression adjustment on matched samples the most robust method. Several different approaches were suggested for the case of multivariate x, on which little or no work has been done.},
author = {Cochran, William G. and Rubin, Donald B.},
doi = {10.2307/25049893},
journal = {Sankhy?: The Indian Journal of Statistics, Series A (1961-2002)},
mendeley-groups = {CARE-paper},
pages = {417--446},
publisher = {Indian Statistical Institute},
title = {Controlling Bias in Observational Studies: A Review},
volume = {35},
year = {1973}
}
@article{Cox1982,
abstract = {An account from first principles is given of a number of aspects of analysis of covariance. Six different meanings of analysis of covariance are outlined and the history of this technique is sketched briefly. The development of the key formulae from the method of least squares is described, and generalizations to other distributions in the exponential family are mentioned. Special problems of application in randomized experiments and in observational studies are discussed. Finally, the decomposition of regression relations is considered along with components of covariance.},
author = {Cox, D. R. and McCullagh, P.},
doi = {10.2307/2530040},
issn = {0006341X},
journal = {Biometrics},
mendeley-groups = {CARE-paper},
month = sep,
number = {3},
pages = {541},
publisher = {International Biometric Society},
title = {A Biometrics Invited Paper with Discussion. Some Aspects of Analysis of Covariance},
volume = {38},
year = {1982}
}
@article{Danielson-Francois2009,
abstract = {Abstract The mate preference characteristics of adult Tetragnatha elongata were assessed with respect to measures of female mass, linear size (length), and condition (mass scaled by length: body condition). Males preferred longer, heavier females and females with higher body condition indices. When mass is partially controlled, males still preferred females of higher body condition, but reversed their preference for length and chose smaller females. We present evidence that female body condition and mass are associated with the volume of her egg load and the proximity of oviposition, whereas female body length is not associated with either. Females displayed no clear preference among males for mass or linear size, but were reluctant to mate in female-choice trials. The small sample size obtained may have obscured the detection of female mate preferences if they exist. This may be the first evidence that mate choice is influenced by body condition rather than mass or linear size among spiders.},
author = {Danielson-Fran{\c{c}}ois, Anne and Fetterer, Christine A. and Smallwood, Peter D.},
doi = {10.1636/0161-8202(2002)030[0020:BCAMCI]2.0.CO;2},
journal = {http://dx.doi.org/10.1636/0161-8202(2002)030[0020:BCAMCI]2.0.CO;2},
keywords = {Mate choice,Tetragnatha elongata,body condition,oviposition,size},
mendeley-groups = {CARE-paper},
month = jan,
title = {BODY CONDITION AND MATE CHOICE IN TETRAGNATHA ELONGATA (ARANEAE, TETRAGNATHIDAE)},
year = {2009}
}
@book{Duncan1975,
address = {New York},
author = {Duncan, Otis Dudley},
isbn = {0-12-224150-9},
mendeley-groups = {CARE-paper},
publisher = {Academic Press, Inc.},
title = {Introduction to Structural Equation Models},
year = {1975}
}
@article{ELKIN2005,
author = {Elkin, C. M. and Reid, M. L.},
doi = {10.1111/j.0269-8463.2005.00935.x},
issn = {0269-8463},
journal = {Functional Ecology},
keywords = {Egg size,Scolytinae,life?history trade?off,parental somatic condition},
mendeley-groups = {CARE-paper},
month = feb,
number = {1},
pages = {102--109},
publisher = {Wiley/Blackwell (10.1111)},
title = {Low energy reserves and energy allocation decisions affect reproduction by Mountain Pine Beetles, Dendroctonus ponderosae},
volume = {19},
year = {2005}
}
@book{Fisher1932,
abstract = {Contains revisions of probability formulas and treatment of correlations. Harvard Book List (edited) 1955 94 (PsycINFO Database Record (c) 2010 APA, all rights reserved)},
address = {Edinburgh},
archivePrefix = {arXiv},
arxivId = {0-05-002170-2},
author = {Fisher, R A},
booktitle = {Biological monographs and manuals},
doi = {10.1056/NEJMc061160},
edition = {4th},
eprint = {0-05-002170-2},
isbn = {0050021702},
issn = {15334406},
mendeley-groups = {CARE-paper},
pmid = {1756371},
publisher = {Oliver and Boyd},
title = {Statistical methods for research workers},
year = {1932}
}
@article{Gail1988,
author = {Gail, M. H. and Tan, W. Y. and Piantadosi, S.},
doi = {10.1093/biomet/75.1.57},
issn = {0006-3444},
journal = {Biometrika},
keywords = {Analysis of covariance,Nonlinear regression,Randomization,Randomized clinical trial,Robust test},
mendeley-groups = {CARE-paper,ZID-paper},
number = {1},
pages = {57--64},
publisher = {Oxford University Press},
title = {Tests for no treatment effect in randomized clinical trials},
volume = {75},
year = {1988}
}
@article{Gail1996,
author = {Gail, Mitchell H. and Mark, Steven D. and Carroll, Raymond J. and Green, Sylvan B. and Pee, David},
doi = {10.1002/(SICI)1097-0258(19960615)15:11<1069::AID-SIM220>3.0.CO;2-Q},
issn = {0277-6715},
journal = {Statistics in Medicine},
mendeley-groups = {CARE-paper},
month = jun,
number = {11},
pages = {1069--1092},
publisher = {John Wiley {\&} Sons, Ltd},
title = {ON DESIGN CONSIDERATIONS AND RANDOMIZATION-BASED INFERENCE FOR COMMUNITY INTERVENTION TRIALS},
volume = {15},
year = {1996}
}
@article{Garcia-Berthou2001,
author = {Garc{\'{i}}a-Berthou, Emili},
doi = {10.1046/j.1365-2656.2001.00524.x},
issn = {00218790},
journal = {Journal of Animal Ecology},
keywords = {analysis of variance,ancova,condition factor,regression analysis,statistical analysis},
mendeley-groups = {CARE-paper},
month = jul,
number = {4},
pages = {708--711},
publisher = {Blackwell Science Ltd},
title = {On the misuse of residuals in ecology: testing regression residuals vs. the analysis of covariance},
volume = {70},
year = {2001}
}
@article{Goldberger1972,
abstract = {This survey of the use of structural equation models and methods by social scientists emphasizes the treatment of unobservable variables and attempts to redress economists' neglect of the work of Sewall Wright},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Goldberger, Arthur S.},
doi = {10.2307/1913851},
eprint = {arXiv:1011.1669v3},
isbn = {9788578110796},
issn = {00129682},
journal = {Econometrica},
mendeley-groups = {CARE-paper},
pages = {979--1001},
pmid = {25246403},
title = {Structural Equation Methods in the Social Sciences},
volume = {40},
year = {1972}
}
@article{Gould1975,
abstract = {Allometry should be defined broadly as the study of size and its consequences, not narrowly as the application of power functions to the data of growth. Variation in size may be ontogenetic, static or phyletic. Errors of omission and treatment have plagued the study of allometry in primates. Standard texts often treat brain size as an independent measure, ignoring its allometric relation with body size - on this basis, gracile australopithecines have been accorded the mental status of gorillas. Intrinsic allometries of the brain/body are likewise neglected: many authors cite cerebral folding as evidence of man's mental superiority, but folding is a mechanical correlate of brain size itself. Confusion among types of scaling heads errors of treatment in both historical primacy [Dubois' ontogenetic inferences from interspecific curves] and current frequency. The predicted parameters of brain-body plots differ greatly for ontogenetic, intrapopulational, interspecific and phyletic allometries. I then discuss basic trends in bivariate allometry at the ordinal level for internal organ weights, skeletal dimensions, lifespan and fetal weight. In considering the causes of basic bivariate allometries, I examine the reason for differences among types of scaling in brain-body relationships. The interspecific exponent of 0.66 strongly suggests a relationship to body surfaces, but we have no satisfactory explanation for why this should be so. The tripartite ontogenetic plot is a consequence of patterns in neuronal differentiation. We do not know why intraspecific exponents fall between 0.2 and 0.4; several partial explanations have been offered. Multivariate techniques have transcended the pictorial representation of transformed coordinates and offer new, powerful approaches to total allometric patterns. Allometry is most often used as a 'criterion for subtraction'. In order to assess the nature and purpose of an adaptation, we must be able to identify and isolate the aspect of its form that depends both upon its size and the size of the body within which it resides. Cranial indices and limb lengths are misinterpreted when authors apply no correction for body size. The search for a criterion of subtraction has been most diligently pursued in studies of the brain. Clearly, brain size must be assessed by comparison with a 'standard' animal of the same body size. But how shall size be measured, especially in fossils; and how shall a standard animal be construed. I discuss and criticize three methods recently used: RADINSKY'S foramen magnum criterion; Jerison's minimum convex polygons and cephalization quotients; and the indices of progression in comparison with 'basal' insectivores' of BAUCHOT, Stephan and their colleagues.},
author = {Gould, Stephen J},
issn = {0301-4231},
journal = {Contributions to primatology},
keywords = {Animals,Biological Evolution,Body Weight,Brain,Cephalometry,Foramen Magnum,Fossils,Haplorhini,Hominidae,Humans,Insectivora,Mathematics,Organ Size,Paleontology,Pan troglodytes,Primates},
mendeley-groups = {CARE-paper},
pages = {244--92},
pmid = {803425},
title = {Allometry in primates, with emphasis on scaling and the evolution of the brain.},
volume = {5},
year = {1975}
}
@article{Green2001,
abstract = {In studies of animal ecology, it is fashionable to use the residuals from an ordinary least squares (OLS) linear regression of body mass against a linear measure of size (the body size indicator, BSI) as an index of body condition. These residual indices are used to study the relationship between condition and reproductive investment, survi-vorship, habitat use, and other parameters. I identify a series of key assumptions underlying the use of this method, each of which is likely to be violated in some or all studies. These assumptions are: (1) that the functional relationship between mass and BSI is linear; (2) that condition is independent of BSI length; (3) that BSI length accurately indicates struc-tural size; (4) that there is no correlation between the size of BSI relative to other structural components (i.e., shape) and the parameter against which the residuals are analyzed; (5) that BSI length is strictly independent of mass; and (6) that BSI length is not subjected to error. Violations of these assumptions place the results of some studies in question and explain the poor relationship observed between OLS residuals and more direct measures of condition. I use avian morphometric data to illustrate how OLS methods can easily lead to Type I and Type II errors through violations of assumptions (5) and (6). Significant relationships reported between OLS residual indices and parameters correlating with body size (e.g., size of sexual ornaments or egg size) are at particular risk of being spurious when the correlation coefficient between mass and BSI is low. Residual indices of condition are often likely to be more reliable when calculated with alternative methods such as nonparametric or model II regression. However, whatever the method used to produce them, residual indices are not suitable for studying the heritability of condition.},
author = {Green, A. J.},
doi = {10.1890/0012-9658(2001)082[1473:MLRMOB]2.0.CO;2},
isbn = {0012-9658},
issn = {00129658},
journal = {Ecology},
keywords = {Avian morphometry,Body mass,Body size indicator,Heritability,Linearity,Nonparametric regressions,Reduced major axis,Regression method,Residual indices of body condition,Statistical artifact,Structural size},
mendeley-groups = {CARE-paper},
number = {5},
pages = {1473--1483},
pmid = {2739},
title = {Mass/length residuals: Measures of body condition or generators of spurious results?},
volume = {82},
year = {2001}
}
@article{Halloran1995,
abstract = {Since the 1970s, Rubin has promoted a model for causal inference based on the potential outcomes if individuals received each of the treatments under study. Commonly, the assumption is made that the outcome in one individual is independent of the treatment assignment and outcome in other individuals. In infectious diseases, however, whether one person become infected is quite often dependent on the infection outcome in other individuals, a situation known as dependent happenings. Here, we review the model proposed by Rubin for the example of infectious disease. Consequences of the violation of the stability assumption include the need for an expanded representation of outcomes, and the existence of different kinds of effects, such as direct and indirect effects. Effects of interest include changes in susceptibility as well as changes in infectiousness. We define the transmission probability formally as an average causal parameter of effect in a population by conditioning on exposure to infection. Unconditional indirect and total effects are difficult to define formally using this model for causal inference. The assignment mechanism can influence the sampling mechanism when it determines who is exposed to infection, raising problems that require further inquiry. We conclude by contrasting the role of differential exposure to infection in direct and indirect effects.},
author = {Halloran, M. E. and Struchiner, C. J.},
doi = {10.1097/00001648-199503000-00010},
isbn = {1044-3983},
issn = {10443983},
journal = {Epidemiology},
keywords = {biological,causality,communicable,diseases,file-import-08-08-19,humans,models},
mendeley-groups = {ZID-paper,CARE-paper},
number = {2},
pages = {142--151},
pmid = {7742400},
title = {Causal inference in infectious diseases.},
volume = {6},
year = {1995}
}
@book{Hayes2009,
author = {Hayes, Richard J. and Moulton, Lawrence H.},
edition = {First},
isbn = {978-1584888161},
pages = {1--338},
publisher = {Chapman and Hall/CRC Press},
title = {{Cluster Randomised Trials}},
year = {2009}
}
@article{Hill1965,
author = {Hill, Austin B},
journal = {Proceedings of the Royal Society of Medicine},
mendeley-groups = {CARE-paper},
month = may,
number = {5},
pages = {295--300},
pmid = {14283879},
publisher = {Royal Society of Medicine Press},
title = {THE ENVIRONMENT AND DISEASE: ASSOCIATION OR CAUSATION?},
volume = {58},
year = {1965}
}
@article{Imai2008,
abstract = {We attempt to clarify, and suggest how to avoid, several serious misunderstandings about and fallacies of causal inference. These issues concern some of the most fundamental advantages and disadvantages of each basic research design. Problems include improper use of hypothesis tests for covariate balance between the treated and control groups, and the consequences of using randomization, blocking before randomization and matching after assignment of treatment to achieve covariate balance. Applied researchers in a wide range of scientific disciplines seem to fall prey to one or more of these fallacies and as a result make suboptimal design or analysis choices. To clarify these points, we derive a new four-part decomposition of the key estimation errors in making causal inferences. We then show how this decomposition can help scholars from different experimental and observational research traditions to understand better each other's inferential problems and attempted solutions.},
author = {Imai, Kosuke and King, Gary and Stuart, Elizabeth A},
journal = {J. R. Statist. Soc. A},
keywords = {Average treatment effects,Blocking,Covariate balance,Matching,Observational studies,Randomized experiments},
mendeley-groups = {CARE-paper},
number = {2},
pages = {481--502},
title = {Misunderstandings between experimentalists and observationalists about causal inference},
volume = {171},
year = {2008}
}
@article{Jakob1996,
abstract = {Behavioral ecologists might often benefit by the ability to directly measure an animal's body condition as an estimate of foraging success, and ultimately fitness. Here we compare the reliability and effectiveness of three indices of body condition that have been heavily used in the morphometrics literature. We examined the ratio index (body mass/body size), the slope-adjusted ratio index (based on regression slopes generated from a reference population), and the residual index (the residuals of a regression of body mass on body size). We present the results of tests performed in the field and laboratory on two ecologically and evolutionarily divergent spider species: the vagrant wolf spider Pardosa milvina (Araneae, Lycosidae), and the colonial orb-weaver Metepeira incrassata (Araneae, Araneidae). The ratio index correlated with body size, which weakened the strength of conclusions that could be drawn. The slope-adjusted ratio index requires an independent and large data set with which to generate the expected values, and was likewise sensitive to body size. The residual index, with appropriate transformations to achieve homoscedasticity, was the most reliable index because it did not vary with body size, and we recommend its general use in behavioral studies that require a condition estimate.},
author = {Jakob, Elizabeth M. and Marshall, Samuel D. and Uetz, George W.},
doi = {10.2307/3545585},
issn = {00301299},
journal = {Oikos},
mendeley-groups = {CARE-paper},
month = oct,
number = {1},
pages = {61},
publisher = {WileyNordic Society Oikos},
title = {Estimating Fitness: A Comparison of Body Condition Indices},
volume = {77},
year = {1996}
}
@article{Kennedy2018,
abstract = {AbstractMost work in causal inference considers deterministic interventions that set each unit's treatment to some fixed value. However, under positivity violations these interventions can lead to non-identification, inefficiency, and effects with little practical relevance. Further, corresponding effects in longitudinal studies are highly sensitive to the curse of dimensionality, resulting in widespread use of unrealistic parametric models. We propose a novel solution to these problems: incremental interventions that shift propensity score values rather than set treatments to fixed values. Incremental interventions have several crucial advantages. First, they avoid positivity assumptions entirely. Second, they require no parametric assumptions and yet still admit a simple characterization of longitudinal effects, independent of the number of timepoints. For example, they allow longitudinal effects to be visualized with a single curve instead of lists of coefficients. After characterizing incremental inte...},
author = {Kennedy, Edward H.},
doi = {10.1080/01621459.2017.1422737},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
keywords = {observational study,positivity,stochastic intervention,time-varying confounding,treatment effect},
mendeley-groups = {CARE-paper},
month = jan,
pages = {0--0},
publisher = {Taylor {\&} Francis},
title = {Nonparametric causal effects based on incremental propensity score interventions},
year = {2018}
}
@article{Knightes2005,
abstract = {A general linear model (GLM) was used to evaluate the deviation of predicted values from expected values for a complex environmental model. For this demonstration, we used the default level interface of the regional mercury cycling model (R-MCM) to simulate epilimnetic total mercury concentrations in Vermont and New Hampshire lakes based on data gathered through the EPAs Regional Environmental Monitoring and Assessment Program (REMAP). The response variable for the GLM was defined as R-MCMs predictive error: the difference between observed mercury concentrations and modeled mercury concentrations in each lake. Least square means of the response variable are used as an estimate of the magnitude and significance of bias, i.e., a statistically discernable trend in predictive errors for a given lake type, e.g., acidic, stratified, or oligotrophic. Using our approach, we determined lake types where significant over-prediction and under-prediction of epilimnetic total mercury concentration was occurring, i.e., regions in parameter space where the model demonstrated significant bias was distinguished from regions where no significant bias existed. This technique is most effective for finding regions of parameter space where bias is significant. Drawing conclusions concerning regions that show no significant bias can be misleading. The significant interaction terms in the GLM demonstrated that addressing this problem using univariate statistical techniques would lead to a loss of important information.},
author = {Knightes, Christopher D. and Cyterski, Michael},
doi = {10.1016/J.ECOLMODEL.2005.01.034},
issn = {0304-3800},
journal = {Ecological Modelling},
mendeley-groups = {CARE-paper},
month = aug,
number = {3},
pages = {366--374},
publisher = {Elsevier},
title = {Evaluating predictive errors of a complex environmental model using a general linear model and least square means},
volume = {186},
year = {2005}
}
@article{Kotiaho2001,
abstract = {Towards a resolution of the lek paradox},
author = {Kotiaho, Janne S. and Simmons, Leigh W. and Tomkins, Joseph L.},
doi = {10.1038/35070557},
issn = {0028-0836},
journal = {Nature},
mendeley-groups = {CARE-paper},
month = apr,
number = {6829},
pages = {684--686},
publisher = {Nature Publishing Group},
title = {Towards a resolution of the lek paradox},
volume = {410},
year = {2001}
}
@book{Lind1753,
address = {Edinburgh},
author = {Lind, James},
mendeley-groups = {CARE-paper},
publisher = {Sands, Murray, and Cochrane for A Kincaid {\&} A Donaldson},
title = {A treatise of the scurvy. In three parts. Containing an inquiry into the nature, causes and cure, of that disease. Together with a critical and chronological view of what has been published on the subject.},
year = {1753}
}
@incollection{Merrill2010,
author = {Merrill, Ray},
booktitle = {Introduction to Epidemiology},
chapter = {2},
isbn = {0763766224},
mendeley-groups = {CARE-paper},
pages = {23--46},
publisher = {Jones {\&} Bartlett Learning},
title = {Historic developments in epidemiology},
year = {2010}
}
@article{Moore2009,
author = {Moore, K. L. and van der Laan, M. J.},
doi = {10.1002/sim.3445},
issn = {02776715},
journal = {Statistics in Medicine},
keywords = {clinical trails,covariate adjustment,efficiency,variable selection},
mendeley-groups = {CARE-paper},
month = jan,
number = {1},
pages = {39--64},
publisher = {Wiley-Blackwell},
title = {Covariate adjustment in randomized trials with binary outcomes: Targeted maximum likelihood estimation},
volume = {28},
year = {2009}
}
@article{Neyman1923,
author = {Neyman, Jerzy},
journal = {Statistical Science},
mendeley-groups = {CARE-paper},
pages = {465--480},
title = {Sur les applications de la theorie des probabilites aux experiences agricoles: Essai des principes},
volume = {5},
year = {1923}
}
@book{Pearl1988,
address = {San Francisco, CA, USA},
author = {Pearl, Judea},
isbn = {1-55860-479-0},
mendeley-groups = {CARE-paper},
publisher = {Morgan Kaufmann Publishers, Inc.},
title = {Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference},
year = {1988}
}
@article{Pearl2010,
abstract = {This paper summarizes recent advances in causal inference and underscores the paradigmatic shifts that must be undertaken in moving from traditional statistical analysis to causal analysis of multivariate data. Special emphasis is placed on the assumptions that underlie all causal inferences, the languages used in formulating those assumptions, the conditional nature of all causal and counterfactual claims, and the methods that have been developed for the assessment of such claims. These advances are illustrated using a general theory of causation based on the Structural Causal Model (SCM) described in Pearl (2000a), which subsumes and unifies other approaches to causation, and provides a coherent mathematical foundation for the analysis of causes and counterfactuals. In particular, the paper surveys the development of mathematical tools for inferring (from a combination of data and assumptions) answers to three types of causal queries: those about (1) the effects of potential interventions, (2) probabilities of counterfactuals, and (3) direct and indirect effects (also known as "mediation"). Finally, the paper defines the formal and conceptual relationships between the structural and potential-outcome frameworks and presents tools for a symbiotic analysis that uses the strong features of both. The tools are demonstrated in the analyses of mediation, causes of effects, and probabilities of causation.},
author = {Pearl, Judea},
doi = {10.2202/1557-4679.1203},
issn = {1557-4679},
journal = {The international journal of biostatistics},
keywords = {causal effects,causes of effects,confounding,counterfactuals,graphical methods,mediation,policy evaluation,potential-outcome,structural equation models},
mendeley-groups = {CARE-paper,ZID-paper},
month = feb,
number = {2},
pages = {Article 7},
pmid = {20305706},
publisher = {Berkeley Electronic Press},
title = {An introduction to causal inference.},
volume = {6},
year = {2010}
}
@book{Pearl2009,
abstract = {This book seeks to integrate research on cause and effect inference from cog-nitive science, econometrics, epidemiology, philosophy, and statistics+ It puts forward the work of its author, his collaborators, and others over the past two decades as a new account of cause and effect inference that can aid practical researchers in many fields, including econometrics+ Pearl adheres to several prop-ositions on cause and effect inference+ Though cause and effect relations are fundamentally deterministic {\~{}}he explicitly excludes quantum mechanical phe-nomena from his concept of cause and effect!, cause and effect analysis in-volves probability language+ Probability language helps to convey uncertainty about cause and effect relations but is insufficient to fully express those rela-tions+ In addition to conditional probabilities of events, cause and effect analy-sis requires graphs or diagrams and a language that distinguishes intervention or manipulation from observation+ Cause and effect analysis also requires coun-terfactual reasoning and causal assumptions in addition to observations and sta-tistical assumptions+ Chapter 1 sketches some of the ingredients of the new approach to cause and effect inference: probability theory, graphs, Bayesian causal networks, causal models, and causal and statistical terminology+ Chapter 2 builds the elements of Chapter 1 into a theory of inferred causation+ Chapter 3 focuses on causal diagrams and identifying causal effects+ Chapter 4 studies intervention or ma-nipulation and direct causal effects+ Chapter 5 considers causality and struc-tural equation models+ Chapter 6 examines Simpson's paradox and confounding+ Chapter 7 blends structural modeling with counterfactual reasoning+ Chapter 8 is an approach to imperfect random assignment experiments through bounding effects and counterfactuals+ Chapter 9 analyzes notions of necessary cause and sufficient cause+ Chapter 10 explicates a concept of single event causality+ The epilogue is a public lecture that Pearl gave at UCLA that, in mostly not too technical language, places the new approach to causality within the long his-tory of thought on the subject+ The interdisciplinary nature of the book, a great strength, at times makes it difficult to read because its theory of inferred causation blends the languages 0266-4666003 {\$}12+00 675 of econometrics and statistics, mathematical graph theory, and Bayesian net-works with philosophical notions of cause and effect+ However, Pearl facili-tates reader understanding by using reasonably straightforward mathematics and examples to help to connect the separate disciplinary discourses+ Nevertheless, an only semiformal approach does not ease sorting the theory into assumptions and deduced conclusions+ The chapters less build one upon the next than take cuts at the subject of causality from slightly different angles+ Hence the reader must struggle to sort assumptions from conclusions and to assemble the theory from the slices+ The book rewards perseverance in such an effort+ Pearl deep-ens one's sense of the nuances involved in cause and effect inference and of-fers intriguing proposals to deal with some unresolved conceptual issues in such inference+ 1. THE BASIC IDEAS OF THE THEORY OF INFERRED CAUSATION},
address = {New York},
author = {Pearl, Judea},
booktitle = {Cambridge University Press},
doi = {10.1017/S0266466603004109},
edition = {2nd},
isbn = {0521773628},
issn = {0266-4666},
mendeley-groups = {CARE-paper},
pmid = {11768929},
publisher = {Cambridge University Press, New York, NY},
title = {Causality: Models, Reasoning, and Inference},
year = {2009}
}
@article{Petersen2010,
abstract = {The assumption of positivity or experimental treatment assignment requires that observed treatment levels vary within confounder strata. This article discusses the positivity assumption in the context of assessing model and parameter-specific identifiability of causal effects. Positivity violations occur when certain subgroups in a sample rarely or never receive some treatments of interest. The resulting sparsity in the data may increase bias with or without an increase in variance and can threaten valid inference. The parametric bootstrap is presented as a tool to assess the severity of such threats and its utility as a diagnostic is explored using simulated and real data. Several approaches for improving the identifiability of parameters in the presence of positivity violations are reviewed. Potential responses to data sparsity include restriction of the covariate adjustment set, use of an alternative projection function to define the target parameter within a marginal structural working model, restriction of the sample, and modification of the target intervention. All of these approaches can be understood as trading off proximity to the initial target of inference for identifiability; we advocate approaching this tradeoff systematically.},
author = {Petersen, Maya L and Porter, Kristin E and Gruber, Susan and Wang, Yue and {Van Der Laan}, Mark J},
doi = {10.1177/0962280210386207},
journal = {Statistical methods in medical research},
keywords = {causal inference,counterfactual,double robust,experimental treatment assignment,inverse probability weight,marginal structural model,parametric bootstrap,positivity,realistic treatment rule,stabilised weights,trimming,truncation},
mendeley-groups = {ZID-paper,CARE-paper},
number = {1},
pages = {31--54},
title = {Diagnosing and responding to violations in the positivity assumption},
volume = {21},
year = {2010}
}
@article{Petersen2014,
abstract = {The practice of epidemiology requires asking causal questions. Formal frameworks for causal inference developed over the past decades have the potential to improve the rigor of this process. However, the appropriate role for formal causal thinking in applied epidemiology remains a matter of debate. We argue that a formal causal framework can help in designing a statistical analysis that comes as close as possible to answering the motivating causal question, while making clear what assumptions are required to endow the resulting estimates with a causal interpretation. A systematic approach for the integration of causal modeling with statistical estimation is presented. We highlight some common points of confusion that occur when causal modeling techniques are applied in practice and provide a broad overview on the types of questions that a causal framework can help to address. Our aims are to argue for the utility of formal causal thinking, to clarify what causal models can and cannot do, and to provide an accessible introduction to the flexible and powerful tools provided by causal models.},
author = {Petersen, Maya L and van der Laan, Mark J},
doi = {10.1097/EDE.0000000000000078},
issn = {1531-5487},
journal = {Epidemiology (Cambridge, Mass.)},
mendeley-groups = {CARE-paper},
month = may,
number = {3},
pages = {418--26},
pmid = {24713881},
publisher = {NIH Public Access},
title = {Causal models and learning from data: integrating causal modeling and statistical estimation.},
volume = {25},
year = {2014}
}
@article{Robins1987,
abstract = {In observational cohort mortality studies with prolonged periods of exposure to the agent under study, independent risk factors for death commonly determine subsequent exposure to the study agent. For example, in occupational mortality studies, date of termination of employment is both a determinant of subsequent exposure to the chemical agent under study (since terminated individuals receive no further exposure) and an independent risk factor for death (since disabled individuals tend to leave employment). When a risk factor determines subsequent exposure and is determined by previous exposure, standard analyses that estimate age-specific mortality rates as a function of cumulative exposure can underestimate the true effect of exposure on mortality, whether or not one adjusts for the risk factor in the analysis. This observation raises the question, ?Which, if any, empirical population parameter can be causally interpreted as the true effect of exposure in observational mortality studies?? In answer, we offer a graphical approach to the identification and estimation of causal parameters in mortality studies with sustained exposure periods. We reanalyze the mortality experience of a cohort of arsenic-exposed copper smelter workers using our approach and compare our results with those obtained using standard methods. We find an adverse effect of arsenic exposure on all cause and lung cancer mortality, which standard methods failed to detect. The analytic approach introduced in this paper may be necessary to control bias in any epidemiologic study in which there exists a risk factor which both determines subsequent exposure and is determined by previous exposure to the agent under study. {\textcopyright} 1987, Pergamon Journals Limited. All rights reserved.},
author = {Robins, James},
doi = {10.1016/S0021-9681(87)80018-8},
isbn = {0021-9681 (Print)$\backslash$r0021-9681 (Linking)},
issn = {00219681},
journal = {Journal of Chronic Diseases},
mendeley-groups = {CARE-paper},
number = {Supplement 2},
pages = {139S--161S},
pmid = {3667861},
title = {A graphical approach to the identification and estimation of causal parameters in mortality studies with sustained exposure periods},
volume = {40},
year = {1987}
}
@article{Robins1986,
author = {Robins, James},
doi = {10.1016/0270-0255(86)90088-6},
issn = {02700255},
journal = {Mathematical Modelling},
mendeley-groups = {Thesis,CARE-paper},
number = {9-12},
pages = {1393--1512},
title = {A new approach to causal inference in mortality studies with a sustained exposure period?application to control of the healthy worker survivor effect},
volume = {7},
year = {1986}
}
@article{Robins2000,
author = {Robins, James M and Hern{\'{a}}n, Miguel {\'{A}}ngel and Brumback, Babette},
journal = {Epidemiology},
mendeley-groups = {CARE-paper,ZID-paper},
number = {5},
pages = {550--560},
title = {Marginal Structural Models and Causal Inference in Epidemiology},
volume = {11},
year = {2000}
}
@article{Rosenbaum1983,
abstract = {The propensity score is the conditional probability of assignment to a particular treatment given a vector of observed covariates. Both large and small sample theory show that adjustment for the scalar propensity score is sufficient to remove bias due to all observed covariates. Applications include: (i) matched sampling on the univariate propensity score, which is a generalization of discriminant matching, (ii) multivariate adjustment by subclassification on the propensity score where the same subclasses are used to estimate treatment effects for all outcome variables and in all subpopulations, and (iii) visual representation of multivariate covariance adjustment by a two- dimensional plot.},
author = {Rosenbaum, Paul R. and Rubin, Donald B.},
doi = {10.1093/biomet/70.1.41},
isbn = {00063444},
issn = {00063444},
journal = {Biometrika},
keywords = {Covariance adjustment,Direct adjustment,Discriminant matching,Matched sampling,Nonrandomized study,Standardization,Stratification,Subclassification},
mendeley-groups = {Thesis,CARE-paper,ZID-paper},
month = apr,
number = {1},
pages = {41--55},
pmid = {16624967},
publisher = {Oxford University Press},
title = {The central role of the propensity score in observational studies for causal effects},
volume = {70},
year = {1983}
}
@article{Rosenblum2010,
abstract = {Models, such as logistic regression and Poisson regression models, are often used to estimate treatment effects in randomized trials. These models leverage information in variables collected before randomization, in order to obtain more precise estimates of treatment effects. However, there is the danger that model misspecification will lead to bias. We show that certain easy to compute, model-based estimators are asymptotically unbiased even when the working model used is arbitrarily misspecified. Furthermore, these estimators are locally efficient. As a special case of our main result, we consider a simple Poisson working model containing only main terms; in this case, we prove the maximum likelihood estimate of the coefficient corresponding to the treatment variable is an asymptotically unbiased estimator of the marginal log rate ratio, even when the working model is arbitrarily misspecified. This is the log-linear analog of ANCOVA for linear models. Our results demonstrate one application of targeted maximum likelihood estimation.},
author = {Rosenblum, Michael and van der Laan, Mark J},
doi = {10.2202/1557-4679.1138},
issn = {1557-4679},
journal = {The international journal of biostatistics},
keywords = {Poisson regression,generalized linear model,misspecified model,targeted maximum likelihood},
mendeley-groups = {CARE-paper},
month = apr,
number = {1},
pages = {Article 13},
pmid = {20628636},
publisher = {Berkeley Electronic Press},
title = {Simple, efficient estimators of treatment effects in randomized trials using generalized linear models to leverage baseline variables.},
volume = {6},
year = {2010}
}
@article{Rubin1990,
abstract = {Of course these do not mean that the formulation I answers (following the frame- work in , 1978 we have that the joint distribution of ( , a2) is},
author = {Rubin, Donald B},
doi = {10.1214/ss/1177012032},
isbn = {08834237},
issn = {0883-4237},
journal = {Statistical Science},
mendeley-groups = {CARE-paper},
number = {4},
pages = {472--480},
pmid = {23507461},
title = {Comment: Neyman (1923) and causal inference in experiments and observational studies},
volume = {5},
year = {1990}
}
@article{Rubin1978,
abstract = {JSTOR is a not-for-profit service that helps scholars, researchers, and students discover, use, and build upon a wide range of content in a trusted digital archive. We use information technology and tools to increase productivity and facilitate new forms of scholarship. For more information about JSTOR, please contact support@jstor.org. Causal effects are comparisons among values that would have been observed under all possible assignments of treatments to experimental units. In an experiment, one assignment of treatments is chosen and only the values under that assignment can be observed. Bayesian inference for causal effects follows from finding the predictive distribution of the values under the other assignments of treatments. This perspective makes clear the role of mechanisms that sample experimental units, assign treatments and record data. Unless these mechanisms are ignorable (known probabi-listic functions of recorded values), the Bayesian must model them in the data analysis and, consequently, confront inferences for causal effects that are sensitive to the specification of the prior distribution of the data. More-over, not all ignorable mechanisms can yield data from which inferences for causal effects are insensitive to prior specifications. Classical random-ized designs stand out as especially appealing assignment mechanisms designed to make inference for causal effects straightforward by limiting the sensitivity of a valid Bayesian analysis.},
author = {Rubin, Donald B.},
doi = {10.1214/aos/1176344064},
isbn = {0090-5364},
issn = {0090-5364},
journal = {The Annals of Statistics},
mendeley-groups = {CARE-paper},
number = {1},
pages = {34--58},
pmid = {531},
title = {Bayesian Inference for Causal Effects: The Role of Randomization},
volume = {6},
year = {1978}
}
@article{Rubin1977,
abstract = {When assignment to treatment group is made solely on the basis of the value of a covariate, X, effort should be concentrated on estimating the conditional expectations of the dependent variable Y given X in the treatment and control groups. One then averages the difference between these conditional expectations over the distribution of X in the relevant population. There is no need for concern about ?other? sources of bias, e.g., unreliability of X, unmeasured background variables. If the conditional expectations are parallel and linear, the proper regression adjustment is the simple covariance adjustment. However, since the quality of the resulting estimates may be sensitive to the adequacy of the underlying model, it is wise to search for nonparallelism and nonlinearity in these conditional expectations. Blocking on the values of X is also appropriate, although the quality of the resulting estimates may be sensitive to the coarseness of the blocking employed. In order for these techniques to be useful i...},
author = {Rubin, Donald B.},
doi = {10.3102/10769986002001001},
issn = {0362-9791},
journal = {Journal of Educational Statistics},
keywords = {Average Treatment Effects,Causal Inference,Covariance Adjustment,Experimental Design,Non-Randomized Studies,Observational Studies,Treatment Assignment},
mendeley-groups = {CARE-paper},
month = mar,
number = {1},
pages = {1--26},
publisher = {SAGE PublicationsSage CA: Thousand Oaks, CA},
title = {Assignment to Treatment Group on the Basis of a Covariate},
volume = {2},
year = {1977}
}
@article{Rubin1974,
author = {Rubin, Donald B.},
doi = {10.1037/h0037350},
issn = {0022-0663},
journal = {Journal of Educational Psychology},
mendeley-groups = {CARE-paper},
number = {5},
pages = {688--701},
title = {Estimating causal effects of treatments in randomized and nonrandomized studies.},
volume = {66},
year = {1974}
}
@article{Sanchez-Chardi2007,
abstract = {We assess the bioaccumulation of metals (Pb, Hg, Cd, Fe, Mg, Zn, Cu, Mn, Mo, Cr) and effects of landfill leachates on morphological (RI, relative weights), plasma (GPT, GOT, creatinine), and genotoxic (MNT) parameters in wood mice, Apodemus sylvaticus, inhabiting close the Garraf landfill site (NE Spain). Due to the high age- and sex-dependent variation in wild populations, we also studied the effect of these biotic factors on the parameters studied. Wood mice from the landfill site, sited in a partially protected area, showed more Cd, Fe, Zn, Cu, Mn, Mo, and Cr than specimens from the reference site. Moreover, mice near the landfill registered low RI and high relative renal weight, GPT, and MN frequency, which indicate that the landfill affects the health of wild mice. In contrast to sympatric shrews from a previous study, wood mice showed lower bioaccumulation of metals and lower variation caused by biotic factors. Moreover, the morphological and physiological alterations demonstrated that they were also more sensitive at environmental pollution. Given the contribution of small mammals to ecosystem function and the scarce ecotoxicological data on the effects of landfill pollution on wild terrestrial mammals, we consider that our study can be used to improve the management of this protected area.},
author = {S{\'{a}}nchez-Chardi, Alejandro and Pe{\~{n}}arroja-Matutano, Cristina and Ribeiro, Ciro Alberto Oliveira and Nadal, Jacint},
doi = {10.1016/J.CHEMOSPHERE.2007.06.047},
issn = {0045-6535},
journal = {Chemosphere},
mendeley-groups = {CARE-paper},
month = nov,
number = {1},
pages = {101--109},
publisher = {Pergamon},
title = {Bioaccumulation of metals and effects of a landfill in small mammals. Part II. The wood mouse, Apodemus sylvaticus},
volume = {70},
year = {2007}
}
@article{Scharfstein1999,
abstract = {Consider a study whose design calls for the study subjects to be followed from enrollment (time t = 0) to time t = T, at which point a primary endpoint of interest Y is to be measured. The design of the study also calls for measurements on a vector V(t) of covariates to be made at one or more times t during the interval [0,T). We are interested in making inferences about the marginal mean J1.0 of Y when some subjects drop out of the study at random times Q prior to the common fixed end of follow-up time T. The purpose of this article is to show how to make inferences about J1.0 when the continuous drop-out time Q is modeled semiparametrically and no restrictions are placed on the joint distribution of the outcome and other measured variables. In particular, we consider two models for the conditional hazard of drop-out given ('V(T), Y), where V(t) denotes the history of the process V(t) through time t, t E [0,T). In the first model, we assume that AQ(tIV(T), Y) = AO(t!V(t» exp(no Y), where no is a scalar parameter and Ao(tIV(t)) is an unrestricted positive function of t and the process V(t). When the process V(t) is high dimensional, estimation in this model is not feasible with moderate sample sizes, due to the curse of dimensionality. For such situations, we consider a second model that imposes the additional restriction that Ao(tIV(t)) = AO(t) exp(,bW(t)), where AO(t) is an unspecified baseline hazard function, W(t) = w(t, V(t», wC,{\textperiodcentered}) is a known function that maps (t, V(t)) to R"; and,o is a q xl unknown parameter vector. When no i' 0, then drop-out is nonignorable. On account of identifiability problems, joint estimation of the mean J1.0 of Y and the selection bias parameter no may be difficult or impossible. Therefore, we propose regarding the selection bias parameter no as known, rather than estimating it from the data. We then perform a sensitivity analysis to see how inference about J1.0 changes as we vary no over a plausible range of values. We apply our approach to the analysis of ACTO 175, an AIDS clinical trial.},
author = {Scharfstein, Daniel O and Rotnitzky, Andrea and Robins, James M},
doi = {10.1080/01621459.1999.10473862},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
keywords = {Augmented inverse probability of censoring weighte,Cox proportional hazards model,Identification,Missing data,Noncompliance,Nonparametric methods,Randomized trials,Sensitivity analysis,Time-dependent covariates},
mendeley-groups = {CARE-paper},
pages = {1096--1120},
title = {Adjusting for Nonignorable Drop-Out Using Semiparametric Nonresponse Models},
volume = {94},
year = {1999}
}
@article{Shen2014,
abstract = {Covariate adjustment in randomized clinical trials has the potential benefit of precision gain. It also has the potential pitfall of reduced objectivity as it opens the possibility of selecting a 'favorable' model that yields strong treatment benefit estimate. Although there is a large volume of statistical literature targeting on the first aspect, realistic solutions to enforce objective inference and improve precision are rare. As a typical randomized trial needs to accommodate many implementation issues beyond statistical considerations, maintaining the objectivity is at least as important as precision gain if not more, particularly from the perspective of the regulatory agencies. In this article, we propose a two-stage estimation procedure based on inverse probability weighting to achieve better precision without compromising objectivity. The procedure is designed in a way such that the covariate adjustment is performed before seeing the outcome, effectively reducing the possibility of selecting a 'favorable' model that yields a strong intervention effect. Both theoretical and numerical properties of the estimation procedure are presented. Application of the proposed method to a real data example is presented.},
author = {Shen, Changyu and Li, Xiaochun and Li, Lingling},
doi = {10.1002/sim.5969},
issn = {1097-0258},
journal = {Statistics in medicine},
keywords = {clinical trials,covariate adjustment,efficiency,inverse probability weighting,objectivity},
mendeley-groups = {CARE-paper},
month = feb,
number = {4},
pages = {555--68},
pmid = {24038458},
publisher = {NIH Public Access},
title = {Inverse probability weighting for covariate adjustment in randomized studies.},
volume = {33},
year = {2014}
}
@book{Snow1855,
address = {New York},
author = {Snow, John},
edition = {2nd},
mendeley-groups = {CARE-paper},
publisher = {Reprinted by Commonwealth Fund, 1936},
title = {On the mode of communication of cholera},
year = {1855}
}
@article{Splawa-Neyman1990,
abstract = {In the portion of the paper translated here, Neyman introduces a model for the analysis of field experiments conducted for the purpose of comparing a number of crop varieties, which makes use of a double-indexed array of unknown potential yields, one index corresponding to varieties and the other to plots. The yield corresponding to only one variety will be observed on any given plot, but through an urn model embodying sampling without replacement from this doubly indexed array, Neyman obtains a formula for the variance of the difference between the averages of the observed yields of two varieties. This variance involves the variance over all plots of the potential yields and the correlation coefficient r between the potential yields of the two varieties on the same plot. Since it is impossible to estimate r directly, Neyman advises taking r = 1, observing that in practice this may lead to using too large an estimated standard deviation, when comparing two variety means.},
author = {Splawa-Neyman, Jerzy and Dabrowska, D. M. and Speed, T. P.},
doi = {10.2307/2245382},
journal = {Statistical Science},
mendeley-groups = {CARE-paper},
number = {4},
pages = {465--472},
publisher = {Institute of Mathematical Statistics},
title = {On the Application of Probability Theory to Agricultural Experiments. Essay on Principles. Section 9.},
volume = {5},
year = {1990}
}
@article{Tsiatis2008,
author = {Tsiatis, Anastasios A. and Davidian, Marie and Zhang, Min and Lu, Xiaomin},
doi = {10.1002/sim.3113},
issn = {02776715},
journal = {Statistics in Medicine},
keywords = {baseline variables,clinical trials,covariate adjustment,efficiency,semiparametric theory,variable selection},
mendeley-groups = {CARE-paper},
month = oct,
number = {23},
pages = {4658--4677},
publisher = {Wiley-Blackwell},
title = {Covariate adjustment for two-sample treatment comparisons in randomized clinical trials: A principled yet flexible approach},
volume = {27},
year = {2008}
}
@book{Rose2011,
abstract = {The statistics profession is at a unique point in history. The need for valid statistical tools is greater than ever; data sets are massive, often measuring hundreds of thousands of measurements for a single subject. The field is ready to move towards clear objective benchmarks under which tools can be evaluated. Targeted learning allows (1) the full generalization and utilization of cross-validation as an estimator selection tool so that the subjective choices made by humans are now made by the machine, and (2) targeting the fitting of the probability distribution of the data toward the target parameter representing the scientific question of interest. This book is aimed at both statisticians and applied researchers interested in causal inference and general effect estimation for observational and experimental data. Part I is an accessible introduction to super learning and the targeted maximum likelihood estimator, including related concepts necessary to understand and apply these methods. Parts II-IX handle complex data structures and topics applied researchers will immediately recognize from their own research, including time-to-event outcomes, direct and indirect effects, positivity violations, case-control studies, censored data, longitudinal data, and genomic studies."Targeted Learning, by Mark J. van der Laan and Sherri Rose, fills a much needed gap in statistical and causal inference. It protects us from wasting computational, analytical, and data resources on irrelevant aspects of a problem and teaches us how to focus on what is relevant answering questions that researchers truly care about."-Judea Pearl, Computer Science Department, University of California, Los Angeles"In summary, this book should be on the shelf of every investigator who conducts observational research and randomized controlled trials. The concepts and methodology are foundational for causal inference and at the same time stay true to what the data at hand can say about the questions that motivate their collection."-Ira B. Tager, Division of Epidemiology, University of California, Berkeley},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {van der Laan, M J and Rose, S},
booktitle = {Targeted Learning: Causal Inference for Observational and Experimental Data},
doi = {10.1007/978-1-4419-9782-1},
eprint = {arXiv:1011.1669v3},
isbn = {0172-7397$\backslash$r978-1-4419-9781-4},
issn = {01727397},
mendeley-groups = {Thesis,CARE-paper},
pages = {3--611},
pmid = {15772297},
publisher = {Springer},
title = {Targeted Learning: Causal Inference for Observational and Experimental Data},
year = {2011}
}
@article{VanderLaan2010,
abstract = {Collaborative double robust targeted maximum likelihood estimators represent a fundamental further advance over standard targeted maximum likelihood estimators of a pathwise differentiable parameter of a data generating distribution in a semiparametric model, introduced in van der Laan, Rubin (2006). The targeted maximum likelihood approach involves fluctuating an initial estimate of a relevant factor (Q) of the density of the observed data, in order to make a bias/variance tradeoff targeted towards the parameter of interest. The fluctuation involves estimation of a nuisance parameter portion of the likelihood, g. TMLE has been shown to be consistent and asymptotically normally distributed (CAN) under regularity conditions, when either one of these two factors of the likelihood of the data is correctly specified, and it is semiparametric efficient if both are correctly specified. In this article we provide a template for applying collaborative targeted maximum likelihood estimation (C-TMLE) to the estimation of pathwise differentiable parameters in semi-parametric models. The procedure creates a sequence of candidate targeted maximum likelihood estimators based on an initial estimate for Q coupled with a succession of increasingly non-parametric estimates for g. In a departure from current state of the art nuisance parameter estimation, C-TMLE estimates of g are constructed based on a loss function for the targeted maximum likelihood estimator of the relevant factor Q that uses the nuisance parameter to carry out the fluctuation, instead of a loss function for the nuisance parameter itself. Likelihood-based cross-validation is used to select the best estimator among all candidate TMLE estimators of Q(0) in this sequence. A penalized-likelihood loss function for Q is suggested when the parameter of interest is borderline-identifiable. We present theoretical results for "collaborative double robustness," demonstrating that the collaborative targeted maximum likelihood estimator is CAN even when Q and g are both mis-specified, providing that g solves a specified score equation implied by the difference between the Q and the true Q(0). This marks an improvement over the current definition of double robustness in the estimating equation literature. We also establish an asymptotic linearity theorem for the C-DR-TMLE of the target parameter, showing that the C-DR-TMLE is more adaptive to the truth, and, as a consequence, can even be super efficient if the first stage density estimator does an excellent job itself with respect to the target parameter. This research provides a template for targeted efficient and robust loss-based learning of a particular target feature of the probability distribution of the data within large (infinite dimensional) semi-parametric models, while still providing statistical inference in terms of confidence intervals and p-values. This research also breaks with a taboo (e.g., in the propensity score literature in the field of causal inference) on using the relevant part of likelihood to fine-tune the fitting of the nuisance parameter/censoring mechanism/treatment mechanism.},
author = {van der Laan, Mark J and Gruber, Susan},
doi = {10.2202/1557-4679.1181},
issn = {1557-4679},
journal = {The international journal of biostatistics},
keywords = {G-computation,asymptotic linearity,causal effect,censored data,coarsening at random,collaborative double robust,crossvalidation,double robust,efficient influence curve,estimating function,estimator selection,influence curve,locally efficient,loss-function,marginal structural model,maximum likelihood estimation,model selection,pathwise derivative,semiparametric model,sieve,super efficiency,super-learning,targeted maximum likelihood estimation,targeted nuisance parameter estimator selection,variable importance},
mendeley-groups = {CARE-paper},
month = may,
number = {1},
pages = {Article 17},
pmid = {20628637},
publisher = {Berkeley Electronic Press},
title = {Collaborative double robust targeted maximum likelihood estimation.},
volume = {6},
year = {2010}
}
@book{vanderLaan2003,
abstract = {They go beyond standard statistical approaches by incorporating all the observed data to allow for informative censoring, to obtain maximal efficiency, and by developing estimators of causal effects. It can be used to teach masters and Ph. D. students in biostatistics and statistics and is suitable for researchers in statistics with a strong interest in the analysis of complex longitudinal data."--Jacket. Motivation, Bibliographic History, and an Overview of the book -- Tour through the General Estimation Problem -- Estimation in a high-dimensional full data model -- The curse of dimensionality in the full data model -- Coarsening at random -- The curse of dimensionality revisited -- The observed data model -- General method for construction of locally efficient estimators -- Comparison with maximum likelihood estimation -- Example: Causal Effect of Air Pollution on Short-Term Asthma Response -- Estimating Functions -- Orthogonal complement of a nuisance tangent space -- Review of efficiency theory -- Estimating functions -- Orthogonal complement of a nuisance tangent space in an observed data model -- Basic useful results to compute projections -- Robustness of Estimating Functions -- Robustness of estimating functions against misspecification of linear convex nuisance parameters -- Double robustness of observed data estimating functions -- Understanding double robustness for a general semiparametric model -- Doubly robust estimation in censored data models -- Using Cross-Validation to Select Nuisance Parameter Models -- A semiparametric model selection criterian -- Forward/backward selection of a nuisance parameter model based on cross-validation with respect to the parameter of interest -- Data analysis example: Estimating the causal relationship between boiled water use and diarrhea in HIV-positive men -- General Methodology -- Full Data Estimating Functions.},
author = {van der Laan, Mark J. and Robins, James M.},
isbn = {0387955569},
mendeley-groups = {CARE-paper},
pages = {397},
publisher = {Springer},
title = {Unified methods for censored longitudinal data and causality},
year = {2003}
}
@article{Vellakkal2013,
abstract = {Background Whether non-communicable diseases (NCDs) are diseases of poverty or affluence in low-and-middle income countries has been vigorously debated. Most analyses of NCDs have used self-reported data, which is biased by differential access to healthcare services between groups of different socioeconomic status (SES). We sought to compare self-reported diagnoses versus standardised measures of NCD prevalence across SES groups in India.  Methods We calculated age-adjusted prevalence rates of common NCDs from the Study on Global Ageing and Adult Health, a nationally representative cross-sectional survey. We compared self-reported diagnoses to standardized measures of disease for five NCDs. We calculated wealth-related and education-related disparities in NCD prevalence by calculating concentration index (C), which ranges from ?1 to +1 (concentration of disease among lower and higher SES groups, respectively).  Findings NCD prevalence was higher (range 5.2 to 19.1{\%}) for standardised measures than self-reported diagnoses (range 3.1 to 9.4{\%}). Several NCDs were particularly concentrated among higher SES groups according to self-reported diagnoses (Csrd) but were concentrated either among lower SES groups or showed no strong socioeconomic gradient using standardized measures (Csm): age-standardised wealth-related C: angina Csrd 0.02 vs. Csm ?0.17; asthma and lung diseases Csrd ?0.05 vs. Csm ?0.04 (age-standardised education-related Csrd 0.04 vs. Csm ?0.05); vision problems Csrd 0.07 vs. Csm ?0.05; depression Csrd 0.07 vs. Csm ?0.13. Indicating similar trends of standardized measures detecting more cases among low SES, concentration of hypertension declined among higher SES (Csrd 0.19 vs. Csm 0.03).  Conclusions The socio-economic patterning of NCD prevalence differs markedly when assessed by standardized criteria versus self-reported diagnoses. NCDs in India are not necessarily diseases of affluence but also of poverty, indicating likely under-diagnosis and under-reporting of diseases among the poor. Standardized measures should be used, wherever feasible, to estimate the true prevalence of NCDs.},
author = {Vellakkal, Sukumar and Subramanian, S. V. and Millett, Christopher and Basu, Sanjay and Stuckler, David and Ebrahim, Shah},
doi = {10.1371/journal.pone.0068219},
editor = {Wiley, Andrea S.},
issn = {1932-6203},
journal = {PLoS ONE},
mendeley-groups = {CARE-paper},
month = jul,
number = {7},
pages = {e68219},
publisher = {Public Library of Science},
title = {Socioeconomic Inequalities in Non-Communicable Diseases Prevalence in India: Disparities between Self-Reported Diagnoses and Standardized Measures},
volume = {8},
year = {2013}
}
@article{Yoshioka1998,
author = {Yoshioka, A},
issn = {0959-8138},
journal = {BMJ (Clinical research ed.)},
mendeley-groups = {CARE-paper},
month = oct,
number = {7167},
pages = {1220--3},
pmid = {9794865},
publisher = {BMJ Publishing Group},
title = {Use of randomisation in the Medical Research Council's clinical trial of streptomycin in pulmonary tuberculosis in the 1940s.},
volume = {317},
year = {1998}
}
@Manual{R2018,
title = {R: A Language and Environment for Statistical Computing},
author = {{R Core Team}},
organization = {R Foundation for Statistical Computing},
address = {Vienna, Austria},
year = {2018},
}
@article{Hill1948,
abstract = {300 Multiple ChoicesThis is a pdf-only article and there is no markup to show you.full-text.pdf},
author = {Marshall, Geoffrey and Blacklock, JWS and Cameron, C and Capon, NB and Cruickshank, R and Gaddum, JH and Heaf, FRG and Hill, AB and Houghton, LE and Hoyle, JC and Raistrick, H},
doi = {10.1136/bmj.2.4582.769},
isbn = {0007-1447 (Print)$\backslash$r0007-1447 (Linking)},
issn = {00071447},
journal = {British Medical Journal},
mendeley-groups = {CARE-paper},
pages = {769--782},
pmid = {18890300},
title = {Streptomycin treatment of pulmonary tuberculosis},
volume = {2},
year = {1948}
}

@article{Hahn1998,
author = {Hahn, Jinyong},
doi = {10.2307/2998560},
issn = {00129682},
journal = {Econometrica},
mendeley-groups = {Thesis},
month = {mar},
number = {2},
pages = {315--331},
title = {{On the Role of the Propensity Score in Efficient Semiparametric Estimation of Average Treatment Effects}},
url = {https://www.jstor.org/stable/2998560?origin=crossref},
volume = {66},
year = {1998}
}

@article{Robins1994,
abstract = {Abstract In applied problems it is common to specify a model for the conditional mean of a response given a set of regressors. A subset of the regressors may be missing for some study subjects either by design or happenstance. In this article we propose a new class of semiparametric estimators, based on inverse probability weighted estimating equations, that are consistent for parameter vector $\alpha$0 of the conditional mean model when the data are missing at random in the sense of Rubin and the missingness probabilities are either known or can be parametrically modeled. We show that the asymptotic variance of the optimal estimator in our class attains the semiparametric variance bound for the model by first showing that our estimation problem is a special case of the general problem of parameter estimation in an arbitrary semiparametric model in which the data are missing at random and the probability of observing complete data is bounded away from 0, and then deriving a representation for the efficient score...},
author = {Robins, James M. and Rotnitzky, Andrea and Zhao, Lue Ping},
doi = {10.1080/01621459.1994.10476818},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
keywords = {Cox proportional hazards model,Linear regression,Logistic regression,Measurement error,Missing covariates,Missing data,Nonlinear regression,Semiparametric efficiency,Survey sampling,Two-stage case-control studies,Validation study},
mendeley-groups = {Thesis},
month = {sep},
number = {427},
pages = {846--866},
publisher = { Taylor {\&} Francis Group },
title = {{Estimation of Regression Coefficients When Some Regressors are not Always Observed}},
url = {https://www.tandfonline.com/doi/full/10.1080/01621459.1994.10476818},
volume = {89},
year = {1994}
}

@article{Kennedy2017,
abstract = {In this paper we give a brief review of semiparametric theory, using as a running example the common problem of estimating an average causal effect. Semiparametric models allow at least part of the data-generating process to be unspecified and unrestricted, and can often yield robust estimators that nonetheless behave similarly to those based on parametric likelihood assumptions, e.g., fast rates of convergence to normal limiting distributions. We discuss the basics of semiparametric theory, focusing on influence functions.},
archivePrefix = {arXiv},
arxivId = {1709.06418},
author = {Kennedy, Edward H.},
eprint = {1709.06418},
file = {:Users/sladmin/Library/Application Support/Mendeley Desktop/Downloaded/Kennedy - 2017 - Semiparametric theory.pdf:pdf},
mendeley-groups = {Thesis},
month = {sep},
title = {{Semiparametric theory}},
url = {http://arxiv.org/abs/1709.06418},
year = {2017}
}

@book{Bickel1993,
abstract = {This book is about estimation in situations where we believe we have$\backslash$nenough knowledge to model some features of the data parametrically,$\backslash$nbut are unwilling to assume anything for other features. Such models$\backslash$nhave arisen in a wide variety of contexts in recent years, particularly$\backslash$nin economics, epidemiology, and astronomy. The complicated structure$\backslash$nof these models typically requires us to consider nonlinear estimation$\backslash$nprocedures which often can only be implemented algorithmically. The$\backslash$ntheory of these procedures is necessarily based on asymptotic approximations.},
address = {Baltimore},
author = {Bickel, Peter J and Klaassen, C AJ and Ritov, Y and Wellner, J A},
doi = {10.1.1.190.4704},
isbn = {0801845416 (acid-free paper)},
issn = {0026-1335},
keywords = {Asymptotic expansions.,Estimation theory.,Mathematical,Models,Statistical inference,model},
pmid = {1689},
publisher = {Johns Hopkins University Press},
title = {{Efficient and adaptive estimation for semiparametric models}},
year = {1993}
}

@article{Horvitz1952,
abstract = {This paper presents a general technique for the treatment of samples drawn without replacement from finite universes when unequal selection probabilities are used. Two sampling schemes are discussed in connection with the problem of determining optimum selection probabilities according to the information available in a supplementary variable. Admittedly, these two schemes have limited application. They should prove useful, however, for the first stage of sampling with multi-stage designs, since both permit unbiased estimation of the sampling variance without resorting to additional assumptions.},
author = {Horvitz, D. G. and Thompson, D. J.},
doi = {10.1080/01621459.1952.10483446},
issn = {1537274X},
journal = {Journal of the American Statistical Association},
mendeley-groups = {CARE-paper},
number = {260},
pages = {663--685},
title = {{A Generalization of Sampling Without Replacement from a Finite Universe}},
volume = {47},
year = {1952}
}

@article{VanderLaan2006,
abstract = {Suppose one observes a sample of independent and identically distributed observations from a particular data generating distribution. Suppose that one is concerned with estimation of a particular pathwise differentiable Euclidean parameter. A substitution estimator evaluating the parameter of a given likelihood based density estimator is typically too biased and might not even converge at the parametric rate: that is, the density estimator was targeted to be a good estimator of the density and might therefore result in a poor estimator of a particular smooth functional of the density. In this article we propose a one step (and, by iteration, k-th step) targeted maximum likelihood density estimator which involves 1) creating a hardest parametric submodel with parameter epsilon through the given density estimator with score equal to the efficient influence curve of the pathwise differentiable parameter at the density estimator, 2) estimating epsilon with the maximum likelihood estimator, and 3) defining a new density estimator as the corresponding update of the original density estimator. We show that iteration of this algorithm results in a targeted maximum likelihood density estimator which solves the efficient influence curve estimating equation and thereby yields a locally efficient estimator of the parameter of interest, under regularity conditions. In particular, we show that, if the parameter is linear and the model is convex, then the targeted maximum likelihood estimator is often achieved in the first step, and it results in a locally efficient estimator at an arbitrary (e.g., heavily misspecified) starting density. We also show that the targeted maximum likelihood estimators are now in full agreement with the locally efficient estimating function methodology as presented in Robins and Rotnitzky (1992) and van der Laan and Robins (2003), creating, in particular, algebraic equivalence between the double robust locally efficient estimators using the targeted maximum likelihood estimators as an estimate of its nuisance parameters, and targeted maximum likelihood estimators. In addition, it is argued that the targeted MLE has various advantages relative to the current estimating function based approach. We proceed by providing data driven methodologies to select the initial density estimator for the targeted MLE, thereby providing data adaptive targeted maximum likelihood estimation methodology. We illustrate the method with various worked out examples.},
author = {van der Laan, Mark J. and Rubin, Daniel},
doi = {10.2202/1557-4679.1043},
file = {:Users/stevelauer/Library/Application Support/Mendeley Desktop/Downloaded/van der Laan, Rubin - 2006 - Targeted Maximum Likelihood Learning.pdf:pdf},
isbn = {15574679},
issn = {15574679},
journal = {International Journal of Biostatistics},
keywords = {Causal effect,Cross-validation,Efficient influence curve,Estimating function,Locally efficient estimation,Loss function,Maximum likelihood estimation,Sieve,Targeted maximum likelihood estimation,Variable importance},
mendeley-groups = {Thesis},
month = {jan},
number = {1},
publisher = {De Gruyter},
title = {{Targeted maximum likelihood learning}},
url = {http://www.degruyter.com/view/j/ijb.2006.2.1/ijb.2006.2.1.1043/ijb.2006.2.1.1043.xml},
volume = {2},
year = {2006}
}

@article{hayes_impact_2019,
	title = {Impact of a universal testing and treatment intervention on {HIV} incidence in {Zambia} and {South} {Africa}: results of the {HPTN} 071 ({PopART}) community-randomized trial},
	issn = {0028-4793},
	shorttitle = {Impact of a universal testing and treatment intervention on {HIV} incidence in {Zambia} and {South} {Africa}},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6587177/},
	doi = {10.1056/NEJMoa1814556},
	abstract = {Background
Universal testing and treatment (UTT) is a potential strategy to reduce HIV incidence, yet prior trial results are inconsistent. We report results from HPTN 071 (PopART), the largest HIV prevention trial to date.

Methods
In this community-randomized trial (2013-18), 21 communities in Zambia and South Africa were randomized to Arm A (PopART intervention, universal antiretroviral therapy [ART]), Arm B (PopART intervention, ART per local guidelines), and Arm C (standard-of-care). The PopART intervention included home-based HIV-testing delivered by community workers who supported linkage-to-care, ART adherence, and other services. The primary outcome, HIV incidence between months 12-36, was measured in a Population Cohort (PC) of {\textasciitilde}2,000 randomly-sampled adults/community aged 18-44y. Viral suppression (VS, {\textless}400 copies HIV RNA/ml) was measured in all HIV-positive PC participants at 24m.

Results
The PC included 48,301 participants. Baseline HIV prevalence was similar across study arms (21\%-22\%). Between months 12-36, 553 incident HIV infections were observed over 39,702 person-years (py; 1.4/100py; women: 1.7/100py; men: 0.8/100py). Adjusted rate-ratios were A vs. C: 0.93 (95\%CI: 0.74-1.18, p=0.51); B vs. C: 0.70 (95\%CI: 0.55-0.88, p=0.006). At 24m, VS was 71.9\% in Arm A; 67.5\% in Arm B; and 60.2\% in Arm C. ART coverage after 36m was 81\% in Arm A and 80\% in Arm B.

Conclusions
The PopART intervention with ART per local guidelines reduced HIV incidence by 30\%. The lack of effect with universal ART was surprising and inconsistent with VS data. This study provides evidence that UTT can reduce HIV incidence at population level.

Trial registration
ClinicalTrials.gov NCT01900977},
	urldate = {2019-08-09},
	journal = {The New England Journal of Medicine},
	author = {Hayes, Richard J. and Donnell, Deborah and Floyd, Sian and Mandla, Nomtha and Bwalya, Justin and Sabapathy, Kalpana and Yang, Blia and Phiri, Mwelwa and Schaap, Ab and Eshleman, Susan H. and Piwowar-Manning, Estelle and Kosloff, Barry and James, Anelet and Skalland, Timothy and Wilson, Ethan and Emel, Lynda and Macleod, David and Dunbar, Rory and Simwinga, Musonda and Makola, Nozizwe and Bond, Virginia and Hoddinott, Graeme and Moore, Ayana and Griffith, Sam and Sista, Nirupama Deshmane and Vermund, Sten H. and El-Sadr, Wafaa and Burns, David N. and Hargreaves, James R. and Hauck, Katharina and Fraser, Christophe and Shanaube, Kwame and Bock, Peter and Beyers, Nulda and Ayles, Helen and Fidler, Sarah},
	month = jul,
	year = {2019},
	pmid = {31314965},
	pmcid = {PMC6587177}
}

@incollection{balzer_tutorial_2016,
	address = {Boca Raton, Florida, USA},
	series = {Handbooks of {Modern} {Statistical} {Methods}},
	title = {Tutorial for causal inference},
	isbn = {978-1-4822-4908-8},
	abstract = {Handbook of Big Data provides a state-of-the-art overview of the analysis of large-scale datasets. Featuring contributions from well-known experts in statistics and computer science, this handbook presents a carefully curated collection of techniques from both industry and academia. Thus, the text instills a working understanding of key statistical},
	language = {en},
	booktitle = {Handbook of {Big} {Data}},
	publisher = {CRC Press},
	author = {Balzer, Laura B. and Petersen, Maya and van der Laan, Mark},
	editor = {Bühlmann, Peter and Drineas, Petros and Kane, Michael and van der Laan, Mark},
	month = feb,
	year = {2016},
	note = {Google-Books-ID: Kx2VCwAAQBAJ},
	keywords = {Business \& Economics / Statistics, Computers / Machine Theory, Mathematics / Probability \& Statistics / General},
	pages = {361--386}
}

@inproceedings{robins2000robust,
  title={Robust estimation in sequentially ignorable missing data and causal inference models},
  author={Robins, James M},
  booktitle={Proceedings of the American Statistical Association},
  volume={1999},
  pages={6--10},
  year={2000},
  organization={Indianapolis, IN}
}
